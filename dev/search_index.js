var documenterSearchIndex = {"docs":
[{"location":"apiref/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"apiref/#Core-Types","page":"API Reference","title":"Core Types","text":"","category":"section"},{"location":"apiref/#Cross-Approximation","page":"API Reference","title":"Cross Approximation","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"CrossApproximation\nACA\nACAᵀ\niACA","category":"page"},{"location":"apiref/#AdaptiveCrossApproximation.ACA","page":"API Reference","title":"AdaptiveCrossApproximation.ACA","text":"ACA{RowPivType,ColPivType,ConvCritType}\n\nAdaptive Cross Approximation (ACA) compressor for low-rank matrix approximation.\n\nComputes M ≈ U * V by iteratively selecting rows and columns (pivots) until a convergence criterion is met. The algorithm starts with row, samples it to select a column pivot, then alternates between row and column selection.\n\nFields\n\nrowpivoting::RowPivType: Strategy for selecting row pivots\ncolumnpivoting::ColPivType: Strategy for selecting column pivots\nconvergence::ConvCritType: Convergence criterion to stop iterations\n\n\n\n\n\n","category":"type"},{"location":"apiref/#AdaptiveCrossApproximation.iACA","page":"API Reference","title":"AdaptiveCrossApproximation.iACA","text":"iACA{RowPivType,ColPivType,ConvCritType}\n\nIncomplete Adaptive Cross Approximation (iACA) compressor.\n\nUnlike standard ACA, iACA computes only half of the factorization. It uses geometric pivoting strategies (e.g., mimicry or tree mimicry) to select row or column pivots based solely on spatial information, making it super efficient for hierarchical matrix construction where only row or column samples are requiered.\n\nFields\n\nrowpivoting::RowPivType: Strategy for selecting row pivots (geometric)\ncolumnpivoting::ColPivType: Strategy for selecting column pivots\nconvergence::ConvCritType: Convergence criterion\n\n\n\n\n\n","category":"type"},{"location":"apiref/#Abstract-Types","page":"API Reference","title":"Abstract Types","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"PivStrat\nPivStratFunctor\nValuePivStrat\nGeoPivStrat\nConvPivStrat\nConvCrit\nConvCritFunctor","category":"page"},{"location":"apiref/#Pivoting-Strategies","page":"API Reference","title":"Pivoting Strategies","text":"","category":"section"},{"location":"apiref/#Value-Based","page":"API Reference","title":"Value-Based","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"MaximumValue\nMaximumValueFunctor\nRandomSamplingPivoting\nRandomSamplingPivotingFunctor","category":"page"},{"location":"apiref/#AdaptiveCrossApproximation.MaximumValue","page":"API Reference","title":"AdaptiveCrossApproximation.MaximumValue","text":"MaximumValue <: ValuePivStrat\n\nPivoting strategy that selects the index with maximum absolute value.\n\nThis is the standard pivoting strategy used in classical ACA algorithms also referred to as partial pivoting. At each iteration, it chooses the row or column with the largest absolute value among the unused indices, ensuring numerical stability and good approximation quality.\n\n\n\n\n\n","category":"type"},{"location":"apiref/#Geometry-Based","page":"API Reference","title":"Geometry-Based","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"FillDistance\nFillDistanceFunctor\nLeja2\nLeja2Functor\nMimicryPivoting\nMimicryPivotingFunctor\nTreeMimicryPivoting\nTreeMimicryPivotingFunctor","category":"page"},{"location":"apiref/#AdaptiveCrossApproximation.FillDistance","page":"API Reference","title":"AdaptiveCrossApproximation.FillDistance","text":"FillDistance{D,F<:Real} <: GeoPivStrat\n\nGeometric pivoting strategy based on fill distance minimization.\n\nSelects pivots to minimize the fill distance, promoting well-distributed sampling in geometric space.\n\nFields\n\n- `pos::Vector{SVector{D,F}}`: Geometric positions of all points (D-dimensional)\n\nType Parameters\n\n- `D`: Spatial dimension\n- `F`: Floating point type for coordinates\n\n\n\n\n\n","category":"type"},{"location":"apiref/#AdaptiveCrossApproximation.Leja2","page":"API Reference","title":"AdaptiveCrossApproximation.Leja2","text":"Leja2{D,F<:Real} <: GeoPivStrat\n\nGeometric pivoting strategy based on Leja points (product of distances).\n\nA modified more efficient version of the fill distance approach. This leads to well-separated point sequences. These points have been introduced as modified leja points and will, therefore, be referred to as Leja2 points within this package.\n\nFields\n\npos::Vector{SVector{D,F}}: Geometric positions of all points (D-dimensional)\n\nType Parameters\n\nD: Spatial dimension\nF: Floating point type for coordinates\n\n\n\n\n\n","category":"type"},{"location":"apiref/#AdaptiveCrossApproximation.MimicryPivoting","page":"API Reference","title":"AdaptiveCrossApproximation.MimicryPivoting","text":"MimicryPivoting{D,F<:Real} <: GeoPivStrat\n\nGeometric pivoting strategy that mimics point distribution of a fully pivoted ACA geometrically.\n\nSelects pivots to reproduce the spatial distribution of a fully pivoted ACA. The strategy balances three objectives: geometric separation (Leja-like behavior), proximity to the reference distribution, and fill distance maximization. Particularly useful for H²–matrix compression where incomplete factorizations are sufficient.\n\nFields\n\nrefpos::Vector{SVector{D,F}}: Positions of test or expansion domain\npos::Vector{SVector{D,F}}: Positions from which to select pivots\n\nType Parameters\n\nD: Spatial dimension\nF: Floating point type for coordinates\n\n\n\n\n\n","category":"type"},{"location":"apiref/#AdaptiveCrossApproximation.TreeMimicryPivoting","page":"API Reference","title":"AdaptiveCrossApproximation.TreeMimicryPivoting","text":"TreeMimicryPivoting{D,T,TreeType} <: GeoPivStrat\n\nTree-aware mimicry pivoting strategy.\n\nThis strategy adapts the MimicryPivoting idea to a hierarchical tree of clusters. Instead of selecting individual points directly, it navigates the tree to pick clusters and then nodes within those clusters so that the selected pivots mimic a reference distribution at multiple scales.\n\nFields\n\nrefpos::Vector{SVector{D,T}}: Reference positions to mimic (e.g., parent pivots)\npos::Vector{SVector{D,T}}: Candidate point positions\ntree::TreeType: Tree structure providing cluster centers, children and values\n\nType parameters\n\nD: spatial dimension\nT: numeric type for coordinates\nTreeType: type of the tree adapter (must implement center, values, children, firstchild)\n\n\n\n\n\n","category":"type"},{"location":"apiref/#Combined","page":"API Reference","title":"Combined","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"CombinedPivStrat\nCombinedPivStratFunctor","category":"page"},{"location":"apiref/#Convergence-Criteria","page":"API Reference","title":"Convergence Criteria","text":"","category":"section"},{"location":"apiref/#Norm-Estimation","page":"API Reference","title":"Norm Estimation","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"FNormEstimator\nFNormEstimatorFunctor\niFNormEstimator\niFNormEstimatorFunctor","category":"page"},{"location":"apiref/#AdaptiveCrossApproximation.FNormEstimator","page":"API Reference","title":"AdaptiveCrossApproximation.FNormEstimator","text":"FNormEstimator{F} <: ConvCrit\n\nFrobenius norm-based convergence criterion for standard ACA. Stops iteration when relative error estimate falls below tolerance.\n\nFields\n\ntol::F: Relative tolerance threshold\n\n\n\n\n\n","category":"type"},{"location":"apiref/#AdaptiveCrossApproximation.iFNormEstimator","page":"API Reference","title":"AdaptiveCrossApproximation.iFNormEstimator","text":"iFNormEstimator{F} <: ConvCrit\n\nFrobenius norm-based convergence criterion for incomplete ACA (iACA). Uses moving average norm estimate for geometric pivoting scenarios.\n\nFields\n\ntol::F: Relative tolerance threshold\n\n\n\n\n\n","category":"type"},{"location":"apiref/#Other-Criteria","page":"API Reference","title":"Other Criteria","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"Extrapolation\nExtrapolationFunctor\nRandomSamplingConv\nRandomSamplingConvFunctor\nCombinedConvCrit\nCombinedConvCritFunctor","category":"page"},{"location":"apiref/#Functions","page":"API Reference","title":"Functions","text":"","category":"section"},{"location":"apiref/#Main-Functions","page":"API Reference","title":"Main Functions","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"aca\nacaᵀ\nnextrc!","category":"page"},{"location":"apiref/#Helper-Functions","page":"API Reference","title":"Helper Functions","text":"","category":"section"},{"location":"apiref/","page":"API Reference","title":"API Reference","text":"leja2!\nnormF!\ntolerance","category":"page"},{"location":"details/iaca/#Incomplete-Adaptive-Cross-Approximation","page":"iACA","title":"Incomplete Adaptive Cross Approximation","text":"","category":"section"},{"location":"details/iaca/#Motivation","page":"iACA","title":"Motivation","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Incomplete ACA (iACA) addresses a fundamental limitation of standard ACA: the requirement to access individual matrix entries. In many applications, particularly hierarchical matrix compression, we need to compress matrix blocks where:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"No entry access: Individual entries A_ij cannot be efficiently computed\nGeometric information available: Point positions or spatial structure is known\nBatch operations only: Can extract full rows/columns but not single entries","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"iACA solves this by using geometric pivoting strategies that select pivots based on spatial distribution rather than matrix values.","category":"page"},{"location":"details/iaca/#The-Challenge","page":"iACA","title":"The Challenge","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Standard ACA requires value-based pivoting to select the next pivot:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"i_k+1 = argmax_i r_ij_k","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"where r_ij_k is a residual entry. Computing this maximum requires:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Access to individual residual entries\nFull deflation: r_ij = A_ij - sum_ell=1^k u_ell i v_ell j","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"For kernel matrices in hierarchical formats:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"A_ij = K(x_i y_j)\nrequires expensive kernel evaluation\nDeflation requires O(k) evaluations per entry\nExamining all entries for maximum is prohibitive","category":"page"},{"location":"details/iaca/#Geometric-Pivoting","page":"iACA","title":"Geometric Pivoting","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Instead of examining matrix values, iACA selects pivots based on geometric criteria applied to the point sets X = x_1 ldots x_m and Y = y_1 ldots y_n.","category":"page"},{"location":"details/iaca/#Fill-Distance-Strategy","page":"iACA","title":"Fill Distance Strategy","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Select points maximally separated from already chosen points:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"x_k+1 = argmax_x in X setminus S_k min_y in S_k x - y","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"This ensures good spatial coverage without matrix access.","category":"page"},{"location":"details/iaca/#Leja-Points","page":"iACA","title":"Leja Points","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Select points to maximize a product criterion:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"x_k+1 = argmax_x in X setminus S_k prod_i=1^k x - x_i","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Provides excellent distribution properties with theoretical guarantees.","category":"page"},{"location":"details/iaca/#Mimicry-Based-Strategies","page":"iACA","title":"Mimicry-Based Strategies","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"The most powerful approach for hierarchical matrices: reuse pivot patterns from previous compressions.","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Key insight: When compressing multiple matrix blocks with similar geometric structure:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Compress first block using any geometric strategy\nStore the selected pivot indices\nReuse these indices for subsequent similar blocks\nAvoid repeated pivot selection computation","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"This dramatically reduces overhead in hierarchical matrix construction where thousands of similar blocks must be compressed.","category":"page"},{"location":"details/iaca/#Algorithm-Variants","page":"iACA","title":"Algorithm Variants","text":"","category":"section"},{"location":"details/iaca/#Row-Matrix-Variant","page":"iACA","title":"Row Matrix Variant","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"For iACA with geometric row pivoting and value-based column pivoting:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Select row geometrically: i_k based on point positions\nExtract row: v_k^T = Ai_k  (full row extraction)\nSelect column by value: j_k = argmax_j v_kj\nExtract and deflate column: tildeu_k = A j_k - sum_ellk u_ell v_ell j_k\nNormalize and continue","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Key property: Only full row/column extractions needed, no individual entry access.","category":"page"},{"location":"details/iaca/#Column-Matrix-Variant","page":"iACA","title":"Column Matrix Variant","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Symmetric variant starting with geometric column selection:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Select column geometrically\nExtract full column\nSelect row by maximum value\nExtract and deflate row\nContinue","category":"page"},{"location":"details/iaca/#Pure-Geometric-Variant","page":"iACA","title":"Pure Geometric Variant","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Both rows and columns selected geometrically:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"No matrix access for pivoting\nMost efficient for hierarchical matrices\nRequires good geometric strategies","category":"page"},{"location":"details/iaca/#Tree-Based-Acceleration","page":"iACA","title":"Tree-Based Acceleration","text":"","category":"section"},{"location":"details/iaca/#Brute-Force-Approach","page":"iACA","title":"Brute Force Approach","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Computing fill distance or Leja criteria naively requires:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"O(mk)\ndistance computations at iteration k\nTotal O(mr^2) for rank r","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"For large point sets, this becomes expensive even though no matrix access is needed.","category":"page"},{"location":"details/iaca/#Tree-Based-Approach","page":"iACA","title":"Tree-Based Approach","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Using spatial trees (quadtree, octree, k-d tree):","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Preprocessing: Build tree structure O(m log m)\nPivot selection: Use tree to find next pivot in O(log m) per iteration\nTotal cost: O(r log m) instead of O(mr^2)","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"The TreeMimicryPivoting strategy implements this efficiently, navigating the tree to quickly identify optimal geometric pivots.","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Additional benefits:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Natural hierarchical structure for matrix blocks\nCan share tree across multiple compressions\nEnables efficient mimicry by tree traversal patterns","category":"page"},{"location":"details/iaca/#Convergence-Criteria","page":"iACA","title":"Convergence Criteria","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Standard convergence criteria require inner products:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"UV^T_F^2 = sum_ij langle u_i u_j rangle langle v_i v_j rangle","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"iACA cannot compute these without entry access. Instead, use simplified criteria:","category":"page"},{"location":"details/iaca/#Moving-Average-Norm","page":"iACA","title":"Moving Average Norm","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Track the moving average:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"barn_k = frac1k sum_i=1^k u_i v_i","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Terminate when:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"u_k v_k  texttol cdot barn_k","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"This requires only row/column norms, not full inner products.","category":"page"},{"location":"details/iaca/#Fixed-Rank","page":"iACA","title":"Fixed Rank","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Simply compress to a predetermined rank based on a priori estimates or hierarchical matrix theory.","category":"page"},{"location":"details/iaca/#Performance-Characteristics","page":"iACA","title":"Performance Characteristics","text":"","category":"section"},{"location":"details/iaca/#Computational-Cost","page":"iACA","title":"Computational Cost","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"For rank-r compression of m times n matrix:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Matrix access: r rows + r columns = O(r(m+n)) entries\nPivot selection: O(r log m) with trees vs O(mr^2) brute force\nDeflation: O(r^2(m+n)) same as standard ACA","category":"page"},{"location":"details/iaca/#Memory","page":"iACA","title":"Memory","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Point coordinates: O(m+n) spatial data\nTree structure: O(m+n) for spatial tree\nBuffers: Same as standard ACA","category":"page"},{"location":"details/iaca/#Accuracy","page":"iACA","title":"Accuracy","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Geometric pivoting generally achieves similar accuracy to value-based pivoting when:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Kernel is smooth (exponentially decaying in distance)\nPoint sets are well-distributed\nSeparation ratio is favorable","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"For rough kernels or poorly distributed points, value-based pivoting may be superior.","category":"page"},{"location":"details/iaca/#Use-Cases","page":"iACA","title":"Use Cases","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Use iACA when:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Working with hierarchical matrix formats (mathcalH, mathcalH^2)\nIndividual kernel evaluations are expensive\nGeometric information is naturally available\nMany similar blocks need compression (use mimicry)","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Use standard ACA when:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Matrix entries are cheap to access\nNo geometric structure available\nMaximum accuracy is critical\nWorking with single matrix compression","category":"page"},{"location":"details/iaca/#Integration-with-Hierarchical-Matrices","page":"iACA","title":"Integration with Hierarchical Matrices","text":"","category":"section"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"iACA is the natural compression method for hierarchical matrices:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"Block clustering: Partition points into spatial clusters\nAdmissibility: Identify far-field block pairs\nCompression: Use iACA with geometric pivoting for each admissible block\nMimicry: Share pivot patterns among geometrically similar blocks\nTree structure: Use same spatial tree for all operations","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"This workflow enables:","category":"page"},{"location":"details/iaca/","page":"iACA","title":"iACA","text":"O(n log n)\nor O(n) storage for n times n matrices\nO(n log n)\nor O(n) matrix-vector products\nEfficient assembly without forming full matrix","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"In order to contribute to this package directly create a pull request against the main branch. Before doing so please: ","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Follow the style of the surrounding code.\nSupplement the documentation.\nWrite tests and check that no errors occur.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"","category":"page"},{"location":"contributing/#Style","page":"Contributing","title":"Style","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"For a consistent style the JuliaFormatter.jl package is used which enforces the style defined in the .JuliaFormatter.toml file. To follow this style simply run","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"using JuliaFormatter\nformat(pkgdir(AdaptiveCrossApproximation))","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nThat all files follow the JuliaFormatter style is tested during the unit tests. Hence, do not forget to execute the two lines above. Otherwise, the tests are likely to not pass.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"","category":"page"},{"location":"contributing/#Documentation","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Add documentation for any changes or new features following the style of the existing documentation. For more information you can have a look at the Documenter.jl documentation.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"","category":"page"},{"location":"contributing/#tests","page":"Contributing","title":"Tests","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Write tests for your code changes and verify that no errors occur, e.g., by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"using Pkg\nPkg.test(\"AdaptiveCrossApproximation.jl\")","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"For a detailed information on which parts are tested the coverage can be evaluated on your local machine, e.g., by","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"using Pkg\nPkg.test(\"AdaptiveCrossApproximation\"; coverage=true, julia_args=[\"-t 4\"])\n\n# determine coverage\nusing Coverage\nsrc_folder = pkgdir(AdaptiveCrossApproximation) * \"/src\"\ncoverage   = process_folder(src_folder)\nLCOV.writefile(\"path-to-folder-you-like\" * \"AdaptiveCrossApproximation.lcov.info\", coverage)\n\nclean_folder(src_folder) # delete .cov files\n\n# extract information about coverage\ncovered_lines, total_lines = get_summary(coverage)\n@info \"Current coverage:\\n$covered_lines of $total_lines lines ($(round(Int, covered_lines / total_lines * 100)) %)\"","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"In Visual Studio Code the Coverage Gutters plugin can be used to visualize the tested lines of the code by inserting the path of the AdaptiveCrossApproximation.lcov.info file in the settings.","category":"page"},{"location":"details/aca/#Adaptive-Cross-Approximation","page":"ACA","title":"Adaptive Cross Approximation","text":"","category":"section"},{"location":"details/aca/#Introduction","page":"ACA","title":"Introduction","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"The Adaptive Cross Approximation (ACA) algorithm computes a low-rank approximation of a matrix A in mathbbR^m times n using only a small subset of its entries. The algorithm builds the approximation:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"A approx UV^T = sum_k=1^r u_k v_k^T","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"where U in mathbbR^m times r and V in mathbbR^n times r are computed iteratively by selecting rows and columns of A.","category":"page"},{"location":"details/aca/#Algorithm","page":"ACA","title":"Algorithm","text":"","category":"section"},{"location":"details/aca/#Standard-ACA-(Row-First)","page":"ACA","title":"Standard ACA (Row-First)","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"The standard ACA algorithm proceeds as follows:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Initialize: Set k = 1, select initial column index j_1\nExtract column: Sample column u_1 = A j_1\nSelect row: Choose row index i_1 (typically i_1 = argmax_i u_1i)\nExtract row: Sample row v_1^T = Ai_1 \nNormalize: Set u_1 leftarrow u_1  v_1j_1\nIterate: For k = 2 3 ldots until convergence:\nSelect column j_k from residual row v_k-1^T\nExtract and deflate column: tildeu_k = A j_k - sum_ell=1^k-1 u_ell v_ell j_k\nSelect row index i_k from tildeu_k\nExtract and deflate row: tildev_k^T = Ai_k  - sum_ell=1^k-1 u_ell i_k v_ell^T\nNormalize: u_k = tildeu_k  tildev_k j_k, v_k = tildev_k\nCheck convergence","category":"page"},{"location":"details/aca/#Column-First-ACA-(ACAᵀ)","page":"ACA","title":"Column-First ACA (ACAᵀ)","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"The column-first variant starts by selecting a column, then a row, reversing the standard order. This can be advantageous when the matrix structure favors column operations.","category":"page"},{"location":"details/aca/#Key-Properties","page":"ACA","title":"Key Properties","text":"","category":"section"},{"location":"details/aca/#Partial-Pivoting","page":"ACA","title":"Partial Pivoting","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"ACA uses partial pivoting: when selecting a row index i_k, only the current column u_k is examined (and vice versa for columns). This makes the algorithm efficient but means it doesn't achieve full pivoting optimality.","category":"page"},{"location":"details/aca/#Computational-Cost","page":"ACA","title":"Computational Cost","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"For an m times n matrix compressed to rank r:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Matrix accesses: r full rows + r full columns = O(r(m+n)) entries\nDeflation cost: O(r^2(m+n)) operations\nTotal: O(r^2(m+n)) assuming matrix access is O(1)","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"This is much cheaper than SVD which requires O(mn min(mn)) operations.","category":"page"},{"location":"details/aca/#Approximation-Quality","page":"ACA","title":"Approximation Quality","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"For matrices with rapidly decaying singular values, ACA produces near-optimal low-rank approximations. The approximation quality depends on:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Matrix rank structure\nPivot selection strategy\nConvergence criterion","category":"page"},{"location":"details/aca/#No-Matrix-Assembly","page":"ACA","title":"No Matrix Assembly","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"The key advantage of ACA is that it never requires the full matrix A to be assembled. Only selected rows and columns are needed, making it ideal for:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Kernel matrices: A_ij = K(x_i y_j) where kernel evaluation is expensive\nBoundary element methods\nHierarchical matrix compression","category":"page"},{"location":"details/aca/#Variants","page":"ACA","title":"Variants","text":"","category":"section"},{"location":"details/aca/#Incomplete-ACA-(iACA)","page":"ACA","title":"Incomplete ACA (iACA)","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Incomplete ACA is designed for scenarios where:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Individual matrix entries cannot be accessed\nOnly geometric information is available\nWorking with hierarchical matrix structures","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Instead of value-based pivoting, iACA uses geometric pivoting strategies that select points based on spatial distribution rather than matrix values. This enables compression of matrix blocks in hierarchical formats without expensive kernel evaluations.","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Key differences:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Pivoting based on geometry, not matrix values\nCannot perform full deflation (no individual entry access)\nUses simplified convergence criteria\nParticularly efficient with tree-based pivoting strategies","category":"page"},{"location":"details/aca/#Fully-Pivoted-ACA","page":"ACA","title":"Fully Pivoted ACA","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Fully pivoted ACA would select each pivot from the entire remaining submatrix rather than just the current row/column. While theoretically optimal, this requires O(mn) work per pivot, defeating the purpose of the algorithm.","category":"page"},{"location":"details/aca/#Mathematical-Foundation","page":"ACA","title":"Mathematical Foundation","text":"","category":"section"},{"location":"details/aca/#Connection-to-Singular-Value-Decomposition","page":"ACA","title":"Connection to Singular Value Decomposition","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"For a matrix with singular value decomposition A = sum_i=1^min(mn) sigma_i u_i v_i^T, the optimal rank-r approximation is:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"A_r^* = sum_i=1^r sigma_i u_i v_i^T","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"ACA produces an approximation A_r^ACA that, under suitable conditions, satisfies:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"A - A_r^ACA_F lesssim C A - A_r^*_F","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"where C is a modest constant depending on the matrix structure.","category":"page"},{"location":"details/aca/#Approximability","page":"ACA","title":"Approximability","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"ACA works well when the matrix has a rapidly decaying singular value spectrum. For matrices arising from smooth kernels, exponential decay is common:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"sigma_k sim e^-alpha k","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"In such cases, ACA with tolerance varepsilon typically requires rank:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"r approx frac1alpha log(1varepsilon)","category":"page"},{"location":"details/aca/#Applications","page":"ACA","title":"Applications","text":"","category":"section"},{"location":"details/aca/#Kernel-Matrices","page":"ACA","title":"Kernel Matrices","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"For kernel matrices A_ij = K(x_i y_j) arising from smooth kernels on separated point clusters:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"ACA avoids computing all O(mn) kernel evaluations\nOnly O(r(m+n)) evaluations needed\nCombined with hierarchical decomposition enables fast methods","category":"page"},{"location":"details/aca/#Boundary-Element-Methods","page":"ACA","title":"Boundary Element Methods","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"In BEM, interaction matrices between well-separated surface patches:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Have low-rank structure\nKernel evaluations are expensive (require numerical integration)\nACA enables efficient assembly and matrix-vector products","category":"page"},{"location":"details/aca/#Hierarchical-Matrices","page":"ACA","title":"Hierarchical Matrices","text":"","category":"section"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"ACA serves as the compression engine in hierarchical matrix formats:","category":"page"},{"location":"details/aca/","page":"ACA","title":"ACA","text":"Compress far-field blocks in mathcalH-matrices\nBuild mathcalH^2-matrix representations\nEnable linear complexity operations for elliptic PDEs","category":"page"},{"location":"#AdaptiveCrossApproximation","page":"Introduction","title":"AdaptiveCrossApproximation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package provides different flavors of the adaptive cross approximation introduced in BEBENDORF. Beside the standard algorithm this package allows to use several different pivoting strategies [] and convergence criteria[]. Further more this package contains an incomplete adaptive cross approximation allowing an efficient pivoting selection for the construction of mathcalH^2-matrices [].","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Installing AdaptiveCrossApproximation is done by entering the package manager (enter ] at the julia REPL) and issuing:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add https://github.com/FastBEAST/AdaptiveCrossApproximation.jl.git","category":"page"},{"location":"#refs","page":"Introduction","title":"References","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The implementation is based on","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"[2] Bauer, M., M. Bebendorf, and B. Feist. “Kernel-Independent Adaptive Construction of mathcal H^2-Matrix Approximations.” Numerische Mathematik 150, no. 1 (January 2022): 1–32. https://doi.org/10.1007/s00211-021-01255-y.\n[3] Heldring, Alexander, Eduard Ubeda, and Juan M. Rius. “Improving the Accuracy of the Adaptive Cross Approximation with a Convergence Criterion Based on Random Sampling.” IEEE Transactions on Antennas and Propagation 69, no. 1 (January 2021): 347–55. https://doi.org/10.1109/TAP.2020.3010857.\n[4] Tetzner, Joshua M., and Simon B. Adrian. “On the Adaptive Cross Approximation for the Magnetic Field Integral Equation.” Preprint. Preprints, January 26, 2024. https://doi.org/10.36227/techrxiv.170630205.56494379/v1.","category":"page"},{"location":"details/convergence/#Convergence-Criteria","page":"Convergence Criteria","title":"Convergence Criteria","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Convergence criteria determine when to stop the ACA iteration, balancing approximation accuracy against computational cost. The choice of convergence criterion affects both the quality and efficiency of the compression.","category":"page"},{"location":"details/convergence/#Overview","page":"Convergence Criteria","title":"Overview","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"During ACA compression, we build a low-rank approximation iteratively:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"A approx UV^T = sum_k=1^r u_k v_k^T","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"After each iteration k, we must decide whether to continue or terminate. This decision is based on estimating the approximation error:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"A - UV^T","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Since computing the exact error requires accessing the full matrix (defeating the purpose of compression), we rely on computable error estimates.","category":"page"},{"location":"details/convergence/#Frobenius-Norm-Estimation","page":"Convergence Criteria","title":"Frobenius Norm Estimation","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"The most common approach estimates the Frobenius norm of the approximation error. The key insight is that we can track:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"UV^T_F^2 = sum_i=1^r sum_j=1^r langle u_i u_j rangle langle v_i v_j rangle","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"This can be computed incrementally as new pivots are added, without accessing the original matrix.","category":"page"},{"location":"details/convergence/#Standard-ACA","page":"Convergence Criteria","title":"Standard ACA","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"For standard ACA with full matrix access, we track the squared Frobenius norm:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"UV^T_F^2 = sum_k=1^r u_k^2 v_k^2 + 2sum_ij langle u_i u_j rangle langle v_i v_j rangle","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"At iteration k, we check if:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"u_k v_k  texttol cdot UV^T_F","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"This provides a relative error estimate. The computation is incremental, adding O(k) work per iteration.","category":"page"},{"location":"details/convergence/#Incomplete-ACA-(iACA)","page":"Convergence Criteria","title":"Incomplete ACA (iACA)","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"For incomplete ACA with geometric pivoting, we cannot compute full inner products. Instead, we use a moving average:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"barn_k = frac1k sum_i=1^k u_i v_i","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"and check if:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"u_k v_k  texttol cdot barn_k","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"This simpler criterion requires only current pivot norms, not historical inner products.","category":"page"},{"location":"details/convergence/#Extrapolation-Based-Criteria","page":"Convergence Criteria","title":"Extrapolation-Based Criteria","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Extrapolation methods attempt to predict the asymptotic behavior of the approximation error by fitting the sequence of pivot norms to a model and extrapolating to estimate future decay rates.","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"The basic idea is to observe that for well-approximable matrices, the pivot contribution u_k v_k often decays exponentially or algebraically:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"u_k v_k sim C rho^k quad textor quad u_k v_k sim C k^-alpha","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"By fitting such a model to recent pivots, we can estimate when the error will drop below the tolerance without computing all pivots.","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Advantages:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Can terminate earlier than norm estimation\nExploits decay structure of the approximation\nUseful for smooth kernel matrices","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Disadvantages:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Requires fitting procedure\nMay be unreliable for irregular decay patterns\nAdditional computational overhead","category":"page"},{"location":"details/convergence/#Random-Sampling","page":"Convergence Criteria","title":"Random Sampling","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Random sampling convergence criteria periodically sample matrix entries to estimate the actual error. This provides direct error feedback but requires matrix access.","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"The algorithm:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Select random matrix entries (ij)\nCompute approximation error A_ij - (UV^T)_ij\nEstimate global error from samples\nTerminate when estimated error < tolerance","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use cases:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"When matrix access is acceptable\nNeed reliable error estimates\nOther criteria may be unreliable","category":"page"},{"location":"details/convergence/#Combined-Criteria","page":"Convergence Criteria","title":"Combined Criteria","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"The CombinedConvCrit allows using multiple convergence checks simultaneously:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Require all criteria to be satisfied (AND logic)\nTerminate when any criterion is met (OR logic)\nUse different criteria for different phases","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"This enables sophisticated stopping strategies, such as:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use norm estimation as primary criterion\nAdd extrapolation for early termination\nInclude maximum rank as safety cutoff","category":"page"},{"location":"details/convergence/#Choosing-a-Criterion","page":"Convergence Criteria","title":"Choosing a Criterion","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use Frobenius norm estimation when:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Standard and well-tested behavior is desired\nWorking with general matrices\nIncremental cost is acceptable","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use extrapolation when:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Matrix has smooth decay properties\nEarly termination is important\nWilling to accept fitting overhead","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use random sampling when:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Need direct error estimates\nMatrix access is cheap\nOther estimates may be unreliable","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Use combined criteria when:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Need robust stopping conditions\nDifferent phases require different checks\nWant safety bounds with optimistic early termination","category":"page"},{"location":"details/convergence/#Practical-Considerations","page":"Convergence Criteria","title":"Practical Considerations","text":"","category":"section"},{"location":"details/convergence/#Tolerance-Selection","page":"Convergence Criteria","title":"Tolerance Selection","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"The tolerance parameter controls the accuracy-rank tradeoff:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Smaller tolerance → higher accuracy, larger rank\nTypical values: 10^-4 to 10^-8\nShould match application requirements","category":"page"},{"location":"details/convergence/#Numerical-Stability","page":"Convergence Criteria","title":"Numerical Stability","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Very small tolerances may encounter numerical issues:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Floating-point precision limits\nIll-conditioned pivot rows/columns\nConsider using combined criteria with maximum rank cutoff","category":"page"},{"location":"details/convergence/#Performance","page":"Convergence Criteria","title":"Performance","text":"","category":"section"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Convergence checking adds overhead:","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"Norm estimation: O(k) per iteration\nExtrapolation: O(k log k) fitting cost\nRandom sampling: depends on sample size","category":"page"},{"location":"details/convergence/","page":"Convergence Criteria","title":"Convergence Criteria","text":"For large problems, the overhead is typically negligible compared to matrix access and linear algebra operations.","category":"page"},{"location":"details/pivoting/#Pivoting-Strategies","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Pivoting strategies determine how rows and columns are selected during ACA compression. The choice of pivoting strategy significantly affects both the accuracy and computational cost of the approximation.","category":"page"},{"location":"details/pivoting/#Overview","page":"Pivoting Strategies","title":"Overview","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"The Adaptive Cross Approximation algorithm requires selecting a sequence of row and column indices (pivots) to build the low-rank approximation:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"A approx sum_k=1^r u_k v_k^T","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"where u_k is a column of A and v_k is a row of A. The quality of the approximation depends critically on the pivot selection strategy.","category":"page"},{"location":"details/pivoting/#Value-Based-Strategies","page":"Pivoting Strategies","title":"Value-Based Strategies","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Value-based strategies select pivots by examining matrix entries to find the most significant components.","category":"page"},{"location":"details/pivoting/#Maximum-Value-Pivoting","page":"Pivoting Strategies","title":"Maximum Value Pivoting","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"The maximum value strategy selects the pivot with the largest absolute value in the current residual. For row selection with a known column j:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"i_k = argmax_i A_ij - sum_ell=1^k-1 u_elli v_ellj","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"This greedy approach often yields good approximations but requires access to individual matrix entries, which may be expensive for kernel matrices or hierarchical applications.","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Advantages:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Simple and intuitive\nOften produces good approximations\nWell-established in the literature","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Disadvantages:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Requires full access to matrix entries\nMay be expensive for kernel evaluations\nCannot exploit geometric structure","category":"page"},{"location":"details/pivoting/#Random-Sampling","page":"Pivoting Strategies","title":"Random Sampling","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Random sampling selects pivots uniformly at random from the remaining indices. While less accurate than maximum value pivoting, it provides several benefits:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"No matrix access needed for pivot selection\nCan be combined with other strategies\nUseful for stochastic algorithms","category":"page"},{"location":"details/pivoting/#Geometry-Based-Strategies","page":"Pivoting Strategies","title":"Geometry-Based Strategies","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Geometry-based strategies exploit spatial information about the underlying point sets, making them ideal for kernel matrices and hierarchical matrix compression where geometric structure is available.","category":"page"},{"location":"details/pivoting/#Fill-Distance","page":"Pivoting Strategies","title":"Fill Distance","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"The fill distance strategy selects points that are maximally separated from already selected points. Given positions X = x_1 ldots x_n and already selected points S_k, the next point is:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"x_k+1 = argmax_x in X setminus S_k min_y in S_k x - y","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"This greedy algorithm approximates an optimal coverage of the point set.","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Properties:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"No matrix access required\nProvides good geometric coverage\nNatural for kernel matrices where A_ij = K(x_i y_j)","category":"page"},{"location":"details/pivoting/#Leja-Points","page":"Pivoting Strategies","title":"Leja Points","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Leja points are selected to maximize a certain product criterion. Starting from an initial point x_1, subsequent points are chosen as:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"x_k+1 = argmax_x in X setminus S_k prod_i=1^k x - x_i","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"This strategy produces well-distributed points that are particularly effective for interpolation problems.","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Advantages:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Strong theoretical foundation\nExcellent distribution properties\nNo matrix evaluations needed","category":"page"},{"location":"details/pivoting/#Mimicry-Pivoting","page":"Pivoting Strategies","title":"Mimicry Pivoting","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Mimicry pivoting reuses pivot patterns from previous compressions, making it highly efficient for hierarchical matrix structures. When compressing similar matrix blocks, the algorithm:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Stores pivot sequences from completed compressions\nReuses these sequences for new blocks with similar structure\nAvoids redundant pivot selection computations","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"This is particularly powerful for hierarchical matrices where many blocks share similar geometric properties.","category":"page"},{"location":"details/pivoting/#Tree-Mimicry-Pivoting","page":"Pivoting Strategies","title":"Tree Mimicry Pivoting","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Tree mimicry pivoting extends the mimicry concept by incorporating hierarchical tree structures. It navigates through a spatial tree (quadtree, octree) to efficiently find and reuse pivot patterns, making it especially efficient for hierarchical matrix compression in multiple dimensions.","category":"page"},{"location":"details/pivoting/#Combined-Strategies","page":"Pivoting Strategies","title":"Combined Strategies","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"The CombinedPivStrat allows mixing different pivoting strategies, enabling hybrid approaches such as:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Start with geometric pivoting for initial coverage\nSwitch to value-based pivoting for refinement\nUse random sampling to break ties or add robustness","category":"page"},{"location":"details/pivoting/#Choosing-a-Strategy","page":"Pivoting Strategies","title":"Choosing a Strategy","text":"","category":"section"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Use value-based strategies when:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Matrix entries are cheap to compute\nMaximum accuracy is required\nGeometric information is unavailable","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Use geometry-based strategies when:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Matrix entries are expensive (kernel evaluations)\nWorking with hierarchical matrices\nGeometric structure is naturally available\nNeed to avoid redundant computations","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Use combined strategies when:","category":"page"},{"location":"details/pivoting/","page":"Pivoting Strategies","title":"Pivoting Strategies","text":"Different phases of compression need different approaches\nWant to balance accuracy and efficiency\nExploiting multiple sources of information","category":"page"}]
}
